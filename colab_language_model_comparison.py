import torch
import torch.nn as nn
from torch.nn import functional as F
import json
import os
import sys
from typing import Iterator, List, Dict, Tuple
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm

# tiktoken ì‚¬ìš©
try:
    import tiktoken
except ImportError as e:
    print(f"tiktoken import ì˜¤ë¥˜: {e}")
    print("í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install tiktoken")
    sys.exit(1)

# í•˜ì´í¼íŒŒë¼ë¯¸í„° (Colabìš©ìœ¼ë¡œ ì¡°ì •)
BATCH_SIZE = 64  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
BLOCK_SIZE = 256  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¦ê°€
MAX_ITERS = 5000  # ë°˜ë³µ íšŸìˆ˜ ê°ì†Œ (ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•´)
EVAL_INTERVAL = 250  # í‰ê°€ ê°„ê²© ì¡°ì •
LEARNING_RATE = 3e-4
EVAL_ITERS = 100  # í‰ê°€ ë°˜ë³µ íšŸìˆ˜ ê°ì†Œ
VOCAB_SIZE = 10000
N_EMBD = 512  # ì„ë² ë”© ì°¨ì› ì¦ê°€
N_HEAD = 8    # ì–´í…ì…˜ í—¤ë“œ ìˆ˜
N_LAYER = 6   # ë ˆì´ì–´ ìˆ˜
DROPOUT = 0.2
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

print(f'ì‚¬ìš© ë””ë°”ì´ìŠ¤: {DEVICE}')
print(f'CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'GPU: {torch.cuda.get_device_name(0)}')

class SimpleTokenizer:
    """ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì € (ë‹¨ì–´ ê¸°ë°˜)"""
    
    def __init__(self, vocab_size: int = VOCAB_SIZE):
        self.vocab_size = vocab_size
        self.word_to_idx = {}
        self.idx_to_word = {}
        self.special_tokens = ["<|endoftext|>", "<|pad|>", "<|unk|>"]
        
    def train_tokenizer(self, text: str, save_path: str = "tokenizer.json"):
        """í…ìŠ¤íŠ¸ì—ì„œ í† í¬ë‚˜ì´ì € í›ˆë ¨"""
        print("í† í¬ë‚˜ì´ì € í›ˆë ¨ ì¤‘...")
        
        # í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ë¡œ ë¶„í• 
        words = text.split()
        
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        word_counts = {}
        for word in words:
            word_counts[word] = word_counts.get(word, 0) + 1
        
        # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì–´íœ˜ ìƒì„±
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
        
        # íŠ¹ìˆ˜ í† í° ì¶”ê°€
        self.word_to_idx = {}
        self.idx_to_word = {}
        
        # íŠ¹ìˆ˜ í† í° ë¨¼ì € ì¶”ê°€
        for i, token in enumerate(self.special_tokens):
            self.word_to_idx[token] = i
            self.idx_to_word[i] = token
        
        # ì¼ë°˜ ë‹¨ì–´ ì¶”ê°€ (vocab_size ì œí•œ)
        vocab_count = len(self.special_tokens)
        for word, count in sorted_words:
            if vocab_count >= self.vocab_size:
                break
            if word not in self.word_to_idx:
                self.word_to_idx[word] = vocab_count
                self.idx_to_word[vocab_count] = word
                vocab_count += 1
        
        # í† í¬ë‚˜ì´ì € ì €ì¥
        tokenizer_data = {
            "word_to_idx": self.word_to_idx,
            "idx_to_word": self.idx_to_word,
            "vocab_size": len(self.word_to_idx)
        }
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(tokenizer_data, f, ensure_ascii=False, indent=2)
        
        print(f"í† í¬ë‚˜ì´ì €ê°€ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ì‹¤ì œ ì–´íœ˜ í¬ê¸°: {len(self.word_to_idx)}")
        
        return len(self.word_to_idx)
    
    def load_tokenizer(self, path: str = "tokenizer.json"):
        """ì €ì¥ëœ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        if os.path.exists(path):
            with open(path, 'r', encoding='utf-8') as f:
                tokenizer_data = json.load(f)
            
            self.word_to_idx = tokenizer_data["word_to_idx"]
            self.idx_to_word = {int(k): v for k, v in tokenizer_data["idx_to_word"].items()}
            print(f"í† í¬ë‚˜ì´ì €ê°€ {path}ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return len(self.word_to_idx)
        else:
            print(f"í† í¬ë‚˜ì´ì € íŒŒì¼ {path}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
    
    def encode(self, text: str) -> List[int]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ì¸ì½”ë”©"""
        if not self.word_to_idx:
            raise ValueError("í† í¬ë‚˜ì´ì €ê°€ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        words = text.split()
        tokens = []
        for word in words:
            if word in self.word_to_idx:
                tokens.append(self.word_to_idx[word])
            else:
                tokens.append(self.word_to_idx["<|unk|>"])
        
        return tokens
    
    def decode(self, tokens: List[int]) -> str:
        """í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©"""
        if not self.idx_to_word:
            raise ValueError("í† í¬ë‚˜ì´ì €ê°€ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        words = []
        for token in tokens:
            if token in self.idx_to_word:
                word = self.idx_to_word[token]
                if word not in self.special_tokens:
                    words.append(word)
        
        return " ".join(words)

class RNNAttention(nn.Module):
    """RNN ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ (LSTM + ì–´í…ì…˜)"""
    
    def __init__(self, vocab_size: int, n_embd: int, n_hidden: int, n_layers: int, dropout: float = 0.2):
        super().__init__()
        self.vocab_size = vocab_size
        self.n_embd = n_embd
        self.n_hidden = n_hidden
        self.n_layers = n_layers
        
        # ì„ë² ë”© ë ˆì´ì–´
        self.token_embedding = nn.Embedding(vocab_size, n_embd)
        self.position_embedding = nn.Embedding(BLOCK_SIZE, n_embd)
        
        # LSTM ë ˆì´ì–´
        self.lstm = nn.LSTM(n_embd, n_hidden, n_layers, batch_first=True, dropout=dropout)
        
        # ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
        self.attention = nn.MultiheadAttention(n_hidden, num_heads=8, dropout=dropout, batch_first=True)
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.ln_f = nn.LayerNorm(n_hidden)
        self.lm_head = nn.Linear(n_hidden, vocab_size)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, idx, targets=None):
        B, T = idx.shape
        
        # ì„ë² ë”©
        tok_emb = self.token_embedding(idx)  # (B, T, n_embd)
        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))  # (T, n_embd)
        x = tok_emb + pos_emb  # (B, T, n_embd)
        x = self.dropout(x)
        
        # LSTM
        lstm_out, _ = self.lstm(x)  # (B, T, n_hidden)
        
        # ì–´í…ì…˜
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)  # (B, T, n_hidden)
        
        # ì”ì°¨ ì—°ê²°
        x = lstm_out + attn_out
        
        # ì¶œë ¥
        x = self.ln_f(x)
        logits = self.lm_head(x)  # (B, T, vocab_size)
        
        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            logits = logits.view(B * T, C)
            targets = targets.view(B * T)
            loss = F.cross_entropy(logits, targets)
        
        return logits, loss
    
    def generate(self, idx, max_new_tokens):
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        for _ in range(max_new_tokens):
            # ì»¨í…ìŠ¤íŠ¸ í¬ë¡­
            idx_cond = idx[:, -BLOCK_SIZE:]
            # ì˜ˆì¸¡
            logits, loss = self(idx_cond)
            # ë§ˆì§€ë§‰ í† í°ë§Œ ì‚¬ìš©
            logits = logits[:, -1, :]
            # í™•ë¥  ë¶„í¬
            probs = F.softmax(logits, dim=-1)
            # ìƒ˜í”Œë§
            idx_next = torch.multinomial(probs, num_samples=1)
            # ì‹œí€€ìŠ¤ì— ì¶”ê°€
            idx = torch.cat((idx, idx_next), dim=1)
        return idx

class TransformerLanguageModel(nn.Module):
    """Transformer ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸"""
    
    def __init__(self, vocab_size: int, n_embd: int, n_head: int, n_layer: int, dropout: float = 0.2):
        super().__init__()
        self.vocab_size = vocab_size
        self.n_embd = n_embd
        
        # ì„ë² ë”© ë ˆì´ì–´
        self.token_embedding = nn.Embedding(vocab_size, n_embd)
        self.position_embedding = nn.Embedding(BLOCK_SIZE, n_embd)
        
        # Transformer ë¸”ë¡ë“¤
        self.blocks = nn.ModuleList([
            TransformerBlock(n_embd, n_head, dropout) for _ in range(n_layer)
        ])
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.ln_f = nn.LayerNorm(n_embd)
        self.lm_head = nn.Linear(n_embd, vocab_size)
        
    def forward(self, idx, targets=None):
        B, T = idx.shape
        
        # ì„ë² ë”©
        tok_emb = self.token_embedding(idx)  # (B, T, n_embd)
        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))  # (T, n_embd)
        x = tok_emb + pos_emb  # (B, T, n_embd)
        
        # Transformer ë¸”ë¡ë“¤
        for block in self.blocks:
            x = block(x)
        
        # ì¶œë ¥
        x = self.ln_f(x)
        logits = self.lm_head(x)  # (B, T, vocab_size)
        
        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            logits = logits.view(B * T, C)
            targets = targets.view(B * T)
            loss = F.cross_entropy(logits, targets)
        
        return logits, loss
    
    def generate(self, idx, max_new_tokens):
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        for _ in range(max_new_tokens):
            # ì»¨í…ìŠ¤íŠ¸ í¬ë¡­
            idx_cond = idx[:, -BLOCK_SIZE:]
            # ì˜ˆì¸¡
            logits, loss = self(idx_cond)
            # ë§ˆì§€ë§‰ í† í°ë§Œ ì‚¬ìš©
            logits = logits[:, -1, :]
            # í™•ë¥  ë¶„í¬
            probs = F.softmax(logits, dim=-1)
            # ìƒ˜í”Œë§
            idx_next = torch.multinomial(probs, num_samples=1)
            # ì‹œí€€ìŠ¤ì— ì¶”ê°€
            idx = torch.cat((idx, idx_next), dim=1)
        return idx

class TransformerBlock(nn.Module):
    """Transformer ë¸”ë¡"""
    
    def __init__(self, n_embd: int, n_head: int, dropout: float = 0.2):
        super().__init__()
        head_size = n_embd // n_head
        self.sa = MultiHeadAttention(n_head, head_size, dropout)
        self.ffwd = FeedForward(n_embd, dropout)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)
        
    def forward(self, x):
        x = x + self.sa(self.ln1(x))
        x = x + self.ffwd(self.ln2(x))
        return x

class MultiHeadAttention(nn.Module):
    """ë©€í‹°í—¤ë“œ ì–´í…ì…˜"""
    
    def __init__(self, num_heads: int, head_size: int, dropout: float = 0.2):
        super().__init__()
        self.heads = nn.ModuleList([Head(head_size, dropout) for _ in range(num_heads)])
        self.proj = nn.Linear(N_EMBD, N_EMBD)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.proj(out)
        out = self.dropout(out)
        return out

class Head(nn.Module):
    """ë‹¨ì¼ ì–´í…ì…˜ í—¤ë“œ"""
    
    def __init__(self, head_size: int, dropout: float = 0.2):
        super().__init__()
        self.key = nn.Linear(N_EMBD, head_size, bias=False)
        self.query = nn.Linear(N_EMBD, head_size, bias=False)
        self.value = nn.Linear(N_EMBD, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        B, T, C = x.shape
        k = self.key(x)
        q = self.query(x)
        v = self.value(x)
        
        wei = q @ k.transpose(-2, -1) * C ** -0.5
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))
        wei = F.softmax(wei, dim=-1)
        wei = self.dropout(wei)
        
        out = wei @ v
        return out

class FeedForward(nn.Module):
    """í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, n_embd: int, dropout: float = 0.2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd),
            nn.Dropout(dropout),
        )
        
    def forward(self, x):
        return self.net(x)

def get_batch(data, batch_size, block_size, device):
    """ë°°ì¹˜ ë°ì´í„° ìƒì„±"""
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i + block_size] for i in ix])
    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])
    x, y = x.to(device), y.to(device)
    return x, y

@torch.no_grad()
def estimate_loss(model, train_data, val_data, eval_iters, device):
    """ì†ì‹¤ ì¶”ì •"""
    out = {}
    model.eval()
    for split, data in [('train', train_data), ('val', val_data)]:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(data, BATCH_SIZE, BLOCK_SIZE, device)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

def train_model(model, train_data, val_data, model_name, max_iters=MAX_ITERS):
    """ëª¨ë¸ í›ˆë ¨"""
    print(f"\n{model_name} ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
    
    model = model.to(DEVICE)
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    
    # í›ˆë ¨ ê¸°ë¡
    train_losses = []
    val_losses = []
    iterations = []
    
    for iter in tqdm(range(max_iters), desc=f"{model_name} í›ˆë ¨"):
        # ì†ì‹¤ í‰ê°€
        if iter % EVAL_INTERVAL == 0 or iter == max_iters - 1:
            losses = estimate_loss(model, train_data, val_data, EVAL_ITERS, DEVICE)
            print(f"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")
            
            train_losses.append(losses['train'].item())
            val_losses.append(losses['val'].item())
            iterations.append(iter)
        
        # ë°°ì¹˜ ìƒ˜í”Œë§
        xb, yb = get_batch(train_data, BATCH_SIZE, BLOCK_SIZE, DEVICE)
        
        # ìˆœì „íŒŒ, ì—­ì „íŒŒ, ìµœì í™”
        logits, loss = model(xb, yb)
        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        optimizer.step()
    
    # ëª¨ë¸ ì €ì¥
    torch.save(model.state_dict(), f'{model_name.lower()}_model.pt')
    print(f"{model_name} ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return train_losses, val_losses, iterations

def generate_text(model, tokenizer, prompt="", max_new_tokens=200):
    """í…ìŠ¤íŠ¸ ìƒì„±"""
    model.eval()
    with torch.no_grad():
        if prompt:
            context = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long, device=DEVICE)
        else:
            context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)
        
        generated = model.generate(context, max_new_tokens)
        generated_text = tokenizer.decode(generated[0].tolist())
        return generated_text

def plot_training_curves(rnn_losses, transformer_losses, iterations):
    """í›ˆë ¨ ê³¡ì„  ì‹œê°í™”"""
    plt.figure(figsize=(15, 6))
    
    plt.subplot(1, 2, 1)
    plt.plot(iterations, rnn_losses[0], label='RNN Train', color='blue', linewidth=2)
    plt.plot(iterations, rnn_losses[1], label='RNN Val', color='blue', linestyle='--', linewidth=2)
    plt.plot(iterations, transformer_losses[0], label='Transformer Train', color='red', linewidth=2)
    plt.plot(iterations, transformer_losses[1], label='Transformer Val', color='red', linestyle='--', linewidth=2)
    plt.xlabel('Iterations', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.title('Training Curves Comparison', fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(iterations, rnn_losses[1], label='RNN Validation', color='blue', linewidth=2)
    plt.plot(iterations, transformer_losses[1], label='Transformer Validation', color='red', linewidth=2)
    plt.xlabel('Iterations', fontsize=12)
    plt.ylabel('Validation Loss', fontsize=12)
    plt.title('Validation Loss Comparison', fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('training_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ì–¸ì–´ ëª¨ë¸ ë¹„êµ ì‹¤í—˜ ì‹œì‘")
    print("=" * 50)
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (Colabì—ì„œ í…ŒìŠ¤íŠ¸ìš©)
    print("ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
    sample_text = """
    To be or not to be, that is the question:
    Whether 'tis nobler in the mind to suffer
    The slings and arrows of outrageous fortune,
    Or to take arms against a sea of troubles,
    And by opposing end them. To dieâ€”to sleep,
    No more; and by a sleep to say we end
    The heart-ache and the thousand natural shocks
    That flesh is heir to: 'tis a consummation
    Devoutly to be wish'd. To die, to sleep;
    To sleep, perchance to dreamâ€”ay, there's the rub:
    For in that sleep of death what dreams may come,
    When we have shuffled off this mortal coil,
    Must give us pauseâ€”there's the respect
    That makes calamity of so long life.
    """ * 100  # ë°˜ë³µí•˜ì—¬ ë” í° ë°ì´í„°ì…‹ ìƒì„±
    
    print(f"ë°ì´í„° í¬ê¸°: {len(sample_text):,} ë¬¸ì")
    
    # í† í¬ë‚˜ì´ì € ì„¤ì •
    tokenizer = SimpleTokenizer(VOCAB_SIZE)
    
    # í† í¬ë‚˜ì´ì € í›ˆë ¨
    vocab_size = tokenizer.train_tokenizer(sample_text, "tokenizer.json")
    
    if vocab_size is None:
        print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨")
        return
    
    # ë°ì´í„° í† í°í™”
    print("ë°ì´í„° í† í°í™” ì¤‘...")
    tokens = tokenizer.encode(sample_text)
    print(f"í† í° ìˆ˜: {len(tokens):,}")
    
    # í›ˆë ¨/ê²€ì¦ ë¶„í• 
    n = int(0.9 * len(tokens))
    train_data = torch.tensor(tokens[:n], dtype=torch.long)
    val_data = torch.tensor(tokens[n:], dtype=torch.long)
    
    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_data):,} í† í°")
    print(f"ê²€ì¦ ë°ì´í„°: {len(val_data):,} í† í°")
    
    # ëª¨ë¸ ìƒì„±
    rnn_model = RNNAttention(vocab_size, N_EMBD, N_EMBD, N_LAYER, DROPOUT)
    transformer_model = TransformerLanguageModel(vocab_size, N_EMBD, N_HEAD, N_LAYER, DROPOUT)
    
    print(f"\nRNN ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in rnn_model.parameters()) / 1e6:.2f}M")
    print(f"Transformer ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in transformer_model.parameters()) / 1e6:.2f}M")
    
    # ëª¨ë¸ í›ˆë ¨
    rnn_train_losses, rnn_val_losses, iterations = train_model(
        rnn_model, train_data, val_data, "RNN"
    )
    
    transformer_train_losses, transformer_val_losses, iterations = train_model(
        transformer_model, train_data, val_data, "Transformer"
    )
    
    # í›ˆë ¨ ê³¡ì„  ì‹œê°í™”
    plot_training_curves(
        (rnn_train_losses, rnn_val_losses),
        (transformer_train_losses, transformer_val_losses),
        iterations
    )
    
    # í…ìŠ¤íŠ¸ ìƒì„± ë¹„êµ
    print("\n=== í…ìŠ¤íŠ¸ ìƒì„± ë¹„êµ ===")
    
    prompt = "To be or not to be"
    print(f"\ní”„ë¡¬í”„íŠ¸: '{prompt}'")
    
    print("\nRNN ëª¨ë¸ ìƒì„±:")
    rnn_text = generate_text(rnn_model, tokenizer, prompt, 100)
    print(rnn_text)
    
    print("\nTransformer ëª¨ë¸ ìƒì„±:")
    transformer_text = generate_text(transformer_model, tokenizer, prompt, 100)
    print(transformer_text)
    
    # ìµœì¢… ì„±ëŠ¥ ë¹„êµ
    print("\n=== ìµœì¢… ì„±ëŠ¥ ë¹„êµ ===")
    print(f"RNN ìµœì¢… ê²€ì¦ ì†ì‹¤: {rnn_val_losses[-1]:.4f}")
    print(f"Transformer ìµœì¢… ê²€ì¦ ì†ì‹¤: {transformer_val_losses[-1]:.4f}")
    
    if transformer_val_losses[-1] < rnn_val_losses[-1]:
        print("âœ… Transformer ëª¨ë¸ì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.")
        improvement = ((rnn_val_losses[-1] - transformer_val_losses[-1]) / rnn_val_losses[-1]) * 100
        print(f"ì„±ëŠ¥ í–¥ìƒ: {improvement:.2f}%")
    else:
        print("âœ… RNN ëª¨ë¸ì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.")
        improvement = ((transformer_val_losses[-1] - rnn_val_losses[-1]) / transformer_val_losses[-1]) * 100
        print(f"ì„±ëŠ¥ í–¥ìƒ: {improvement:.2f}%")
    
    print("\nğŸ‰ ì‹¤í—˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()

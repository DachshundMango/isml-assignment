# ê³¼ì œ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ì–¸ì–´ ëª¨ë¸ ë¹„êµ í”„ë¡œê·¸ë¨
# - BPE í† í¬ë‚˜ì´ì € ì‚¬ìš© (vocab size 10000)
# - RNN (LSTM)ê³¼ Transformer ëª¨ë¸ ë¹„êµ
# - ì›ë³¸ character-level ëª¨ë¸ì„ word-levelë¡œ ë³€í™˜

import torch
import torch.nn as nn
from torch.nn import functional as F
import json
import os
import sys
from typing import Iterator, List, Dict, Tuple
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import re

# BPEasy ì‚¬ìš©ì„ ìœ„í•œ ì„¤ì • (ë¡œì»¬ bpeasy ì‚¬ìš©)
try:
    # ë¡œì»¬ bpeasy í´ë”ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
    import sys
    import os
    bpeasy_path = os.path.join(os.getcwd(), 'bpeasy')
    if os.path.exists(bpeasy_path):
        sys.path.insert(0, bpeasy_path)
        from bpeasy.tokenizer import BPEasyTokenizer
        print("âœ… ë¡œì»¬ BPEasy ì‚¬ìš© ê°€ëŠ¥")
    else:
        print("âŒ ë¡œì»¬ bpeasy í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        BPEasyTokenizer = None
except ImportError as e:
    print(f"âŒ BPEasy import ì‹¤íŒ¨: {e}")
    print("ëŒ€ì•ˆ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    BPEasyTokenizer = None

# í•˜ì´í¼íŒŒë¼ë¯¸í„° (ì›ë³¸ ëª¨ë¸ ê¸°ë°˜)
BATCH_SIZE = 64
BLOCK_SIZE = 256  # ì›ë³¸ê³¼ ë™ì¼
MAX_ITERS = 5000  # ìˆ˜ë ´ê¹Œì§€ í›ˆë ¨
EVAL_INTERVAL = 500
LEARNING_RATE = 3e-4
EVAL_ITERS = 200
VOCAB_SIZE = 10000  # ê³¼ì œ ìš”êµ¬ì‚¬í•­
N_EMBD = 384  # ì›ë³¸ê³¼ ë™ì¼
N_HEAD = 6    # ì›ë³¸ê³¼ ë™ì¼
N_LAYER = 6   # ì›ë³¸ê³¼ ë™ì¼
DROPOUT = 0.2
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

print(f'ì‚¬ìš© ë””ë°”ì´ìŠ¤: {DEVICE}')
print(f'CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'GPU: {torch.cuda.get_device_name(0)}')

class BPETokenizer:
    """BPE í† í¬ë‚˜ì´ì € (ê³¼ì œ ìš”êµ¬ì‚¬í•­ ì¶©ì¡±)"""
    
    def __init__(self, vocab_size: int = VOCAB_SIZE):
        self.vocab_size = vocab_size
        self.tokenizer = None
        self.special_tokens = ["<|endoftext|>", "<|pad|>", "<|unk|>"]
        
    def train_tokenizer(self, text: str, save_path: str = "bpe_tokenizer.json"):
        """BPE í† í¬ë‚˜ì´ì € í›ˆë ¨"""
        print("BPE í† í¬ë‚˜ì´ì € í›ˆë ¨ ì¤‘...")
        
        if BPEasyTokenizer is not None:
            try:
                # BPEasy ì‚¬ìš© - ì‹¤ì œ BPE í›ˆë ¨
                print("ì‹¤ì œ BPE í† í¬ë‚˜ì´ì € í›ˆë ¨ ì¤‘...")
                
                # í…ìŠ¤íŠ¸ë¥¼ ì´í„°ë ˆì´í„°ë¡œ ë³€í™˜ (BPEasy ìš”êµ¬ì‚¬í•­)
                text_lines = text.split('\n')
                text_iterator = iter(text_lines)
                
                # BPEasy í† í¬ë‚˜ì´ì € í›ˆë ¨
                self.tokenizer = BPEasyTokenizer.train(
                    iterator=text_iterator,
                    vocab_size=self.vocab_size,
                    max_token_length=128,
                    regex_pattern=r"""[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+""",
                    special_tokens=self.special_tokens,
                    name="bpeasy_assignment"
                )
                
                print(f"âœ… ì‹¤ì œ BPE í† í¬ë‚˜ì´ì € í›ˆë ¨ ì™„ë£Œ!")
                print(f"   ì–´íœ˜ í¬ê¸°: {len(self.tokenizer)}")
                print(f"   íŠ¹ìˆ˜ í† í°: {self.special_tokens}")
                
                # í† í¬ë‚˜ì´ì € ì €ì¥
                self.tokenizer.save(save_path.replace('.json', '_bpeasy.json'))
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                tokenizer_data = {
                    "vocab_size": len(self.tokenizer),
                    "special_tokens": self.special_tokens,
                    "bpe_model": "bpeasy",
                    "actual_vocab_size": len(self.tokenizer)
                }
                
                with open(save_path, 'w', encoding='utf-8') as f:
                    json.dump(tokenizer_data, f, ensure_ascii=False, indent=2)
                
                return len(self.tokenizer)
                
            except Exception as e:
                print(f"BPEasy í›ˆë ¨ ì‹¤íŒ¨: {e}")
                print("ëŒ€ì•ˆ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # ëŒ€ì•ˆ: ê°œì„ ëœ ë‹¨ì–´ ê¸°ë°˜ í† í¬ë‚˜ì´ì € (BPE ìŠ¤íƒ€ì¼)
        return self._train_fallback_tokenizer(text, save_path)
    
    def _train_fallback_tokenizer(self, text: str, save_path: str):
        """ëŒ€ì•ˆ í† í¬ë‚˜ì´ì € (BPE ìŠ¤íƒ€ì¼ êµ¬í˜„)"""
        print("ëŒ€ì•ˆ í† í¬ë‚˜ì´ì € í›ˆë ¨ ì¤‘...")
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        text = re.sub(r'\s+', ' ', text)  # ê³µë°± ì •ê·œí™”
        words = text.split()
        
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        word_counts = {}
        for word in words:
            word_counts[word] = word_counts.get(word, 0) + 1
        
        # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì–´íœ˜ ìƒì„±
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
        
        # í† í¬ë‚˜ì´ì € ë°ì´í„° ìƒì„±
        self.word_to_idx = {}
        self.idx_to_word = {}
        
        # íŠ¹ìˆ˜ í† í° ë¨¼ì € ì¶”ê°€
        for i, token in enumerate(self.special_tokens):
            self.word_to_idx[token] = i
            self.idx_to_word[i] = token
        
        # ì¼ë°˜ ë‹¨ì–´ ì¶”ê°€ (vocab_size ì œí•œ)
        vocab_count = len(self.special_tokens)
        for word, count in sorted_words:
            if vocab_count >= self.vocab_size:
                break
            if word not in self.word_to_idx:
                self.word_to_idx[word] = vocab_count
                self.idx_to_word[vocab_count] = word
                vocab_count += 1
        
        # í† í¬ë‚˜ì´ì € ì €ì¥
        tokenizer_data = {
            "word_to_idx": self.word_to_idx,
            "idx_to_word": self.idx_to_word,
            "vocab_size": len(self.word_to_idx),
            "special_tokens": self.special_tokens,
            "bpe_model": "fallback"
        }
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(tokenizer_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ëŒ€ì•ˆ í† í¬ë‚˜ì´ì € í›ˆë ¨ ì™„ë£Œ (ì–´íœ˜ í¬ê¸°: {len(self.word_to_idx)})")
        return len(self.word_to_idx)
    
    def load_tokenizer(self, path: str = "bpe_tokenizer.json"):
        """ì €ì¥ëœ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        if os.path.exists(path):
            with open(path, 'r', encoding='utf-8') as f:
                tokenizer_data = json.load(f)
            
            if tokenizer_data.get("bpe_model") == "bpeasy" and BPEasyTokenizer is not None:
                # BPEasy í† í¬ë‚˜ì´ì € ë¡œë“œ
                bpeasy_path = path.replace('.json', '_bpeasy.json')
                if os.path.exists(bpeasy_path):
                    self.tokenizer = BPEasyTokenizer.from_file(bpeasy_path)
                    print(f"âœ… ì‹¤ì œ BPEasy í† í¬ë‚˜ì´ì €ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    print(f"   ì–´íœ˜ í¬ê¸°: {len(self.tokenizer)}")
                    return len(self.tokenizer)
                else:
                    print(f"BPEasy ëª¨ë¸ íŒŒì¼ {bpeasy_path}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return None
            else:
                # ëŒ€ì•ˆ í† í¬ë‚˜ì´ì € ë¡œë“œ
                self.word_to_idx = tokenizer_data["word_to_idx"]
                self.idx_to_word = {int(k): v for k, v in tokenizer_data["idx_to_word"].items()}
                print(f"ëŒ€ì•ˆ í† í¬ë‚˜ì´ì €ê°€ {path}ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return len(self.word_to_idx)
        else:
            print(f"í† í¬ë‚˜ì´ì € íŒŒì¼ {path}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
    
    def encode(self, text: str) -> List[int]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ì¸ì½”ë”©"""
        if self.tokenizer is not None and BPEasyTokenizer is not None:
            # BPEasy ì‚¬ìš©
            return self.tokenizer.encode(text)
        else:
            # ëŒ€ì•ˆ í† í¬ë‚˜ì´ì € ì‚¬ìš©
            if not hasattr(self, 'word_to_idx'):
                raise ValueError("í† í¬ë‚˜ì´ì €ê°€ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            words = text.split()
            tokens = []
            for word in words:
                if word in self.word_to_idx:
                    tokens.append(self.word_to_idx[word])
                else:
                    tokens.append(self.word_to_idx["<|unk|>"])
            
            return tokens
    
    def decode(self, tokens: List[int]) -> str:
        """í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©"""
        if self.tokenizer is not None and BPEasyTokenizer is not None:
            # BPEasy ì‚¬ìš©
            return self.tokenizer.decode(tokens)
        else:
            # ëŒ€ì•ˆ í† í¬ë‚˜ì´ì € ì‚¬ìš©
            if not hasattr(self, 'idx_to_word'):
                raise ValueError("í† í¬ë‚˜ì´ì €ê°€ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            words = []
            for token in tokens:
                if token in self.idx_to_word:
                    word = self.idx_to_word[token]
                    if word not in self.special_tokens:
                        words.append(word)
            
            return " ".join(words)

# ì›ë³¸ ëª¨ë¸ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ RNN ëª¨ë¸ (LSTM)
class RNNAttention(nn.Module):
    """RNN ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ (LSTM + ì–´í…ì…˜) - ì›ë³¸ êµ¬ì¡° ê¸°ë°˜"""
    
    def __init__(self, vocab_size: int):
        super().__init__()
        self.vocab_size = vocab_size
        
        # ì›ë³¸ ëª¨ë¸ê³¼ ë™ì¼í•œ ì„ë² ë”© êµ¬ì¡°
        self.token_embedding_table = nn.Embedding(vocab_size, N_EMBD)
        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBD)
        
        # LSTM ë ˆì´ì–´ (RNN êµ¬í˜„)
        self.lstm = nn.LSTM(N_EMBD, N_EMBD, N_LAYER, batch_first=True, dropout=DROPOUT)
        
        # ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
        self.attention = nn.MultiheadAttention(N_EMBD, num_heads=N_HEAD, dropout=DROPOUT, batch_first=True)
        
        # ì¶œë ¥ ë ˆì´ì–´ (ì›ë³¸ê³¼ ë™ì¼)
        self.ln_f = nn.LayerNorm(N_EMBD)
        self.lm_head = nn.Linear(N_EMBD, vocab_size)
        
    def forward(self, idx: torch.Tensor, targets: torch.Tensor = None):
        B, T = idx.shape
        
        # ì„ë² ë”© (ì›ë³¸ê³¼ ë™ì¼)
        tok_emb = self.token_embedding_table(idx)  # (B, T, C)
        pos_emb = self.position_embedding_table(torch.arange(T, device=DEVICE))  # (T, C)
        x = tok_emb + pos_emb  # (B, T, C)
        
        # LSTM ì²˜ë¦¬
        lstm_out, _ = self.lstm(x)  # (B, T, C)
        
        # ì–´í…ì…˜ ì ìš©
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)  # (B, T, C)
        
        # ì¶œë ¥ ë ˆì´ì–´ (ì›ë³¸ê³¼ ë™ì¼)
        x = self.ln_f(attn_out)
        logits = self.lm_head(x)  # (B, T, vocab_size)
        
        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            logits = logits.view(B * T, C)
            targets = targets.view(B * T)
            loss = F.cross_entropy(logits, targets)
        
        return logits, loss
    
    def generate(self, idx: torch.Tensor, max_new_tokens: int):
        """í…ìŠ¤íŠ¸ ìƒì„± (ì›ë³¸ê³¼ ë™ì¼í•œ ë°©ì‹)"""
        for _ in range(max_new_tokens):
            # ì»¨í…ìŠ¤íŠ¸ í¬ê¸° ì œí•œ
            idx_cond = idx[:, -BLOCK_SIZE:]
            # ì˜ˆì¸¡
            logits, loss = self(idx_cond)
            # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ë§Œ ì‚¬ìš©
            logits = logits[:, -1, :]  # (B, C)
            # í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜
            probs = F.softmax(logits, dim=-1)  # (B, C)
            # ë‹¤ìŒ í† í° ìƒ˜í”Œë§
            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)
            # ì‹œí€€ìŠ¤ì— ì¶”ê°€
            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)
        return idx

# ì›ë³¸ ëª¨ë¸ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ Transformer ëª¨ë¸
class Head(nn.Module):
    """ì›ë³¸ ëª¨ë¸ì˜ Head í´ë˜ìŠ¤"""
    def __init__(self, head_size):
        super().__init__()
        self.key = nn.Linear(N_EMBD, head_size, bias=False)
        self.query = nn.Linear(N_EMBD, head_size, bias=False)
        self.value = nn.Linear(N_EMBD, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))
        self.dropout = nn.Dropout(DROPOUT)

    def forward(self, x):
        B, T, C = x.shape
        k = self.key(x)   # (B, T, head_size)
        q = self.query(x) # (B, T, head_size)
        v = self.value(x) # (B, T, head_size)
        # compute attention scores ("affinities")
        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, T)
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)
        wei = F.softmax(wei, dim=-1) # (B, T, T)
        wei = self.dropout(wei)
        # perform the weighted aggregation of the values
        out = wei @ v # (B, T, head_size)
        return out

class MultiHeadAttention(nn.Module):
    """ì›ë³¸ ëª¨ë¸ì˜ MultiHeadAttention í´ë˜ìŠ¤"""
    def __init__(self, num_heads, head_size):
        super().__init__()
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
        self.proj = nn.Linear(N_EMBD, N_EMBD)
        self.dropout = nn.Dropout(DROPOUT)

    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.proj(out)
        out = self.dropout(out)
        return out

class FeedForward(nn.Module):
    """ì›ë³¸ ëª¨ë¸ì˜ FeedForward í´ë˜ìŠ¤"""
    def __init__(self, n_embd):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd),
            nn.Dropout(DROPOUT),
        )

    def forward(self, x):
        return self.net(x)

class TransformerBlock(nn.Module):
    """ì›ë³¸ ëª¨ë¸ì˜ TransformerBlock í´ë˜ìŠ¤"""
    def __init__(self, n_embd, n_head):
        super().__init__()
        head_size = n_embd // n_head
        self.sa = MultiHeadAttention(n_head, head_size)
        self.ffwd = FeedForward(n_embd)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)

    def forward(self, x):
        x = x + self.sa(self.ln1(x))
        x = x + self.ffwd(self.ln2(x))
        return x

class TransformerLanguageModel(nn.Module):
    """ì›ë³¸ ëª¨ë¸ì˜ TransformerLanguageModel í´ë˜ìŠ¤ (word-levelë¡œ ë³€í™˜)"""
    def __init__(self, vocab_size: int):
        super().__init__()
        # ê° í† í°ì€ vocab_sizeì˜ ì–´íœ˜ì—ì„œ ë‚˜ì˜µë‹ˆë‹¤
        self.token_embedding_table = nn.Embedding(vocab_size, N_EMBD)
        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBD)
        self.blocks = nn.Sequential(*[TransformerBlock(N_EMBD, n_head=N_HEAD) for _ in range(N_LAYER)])
        self.ln_f = nn.LayerNorm(N_EMBD) # final layer norm
        self.lm_head = nn.Linear(N_EMBD, vocab_size)

    def forward(self, idx: torch.Tensor, targets: torch.Tensor = None):
        B, T = idx.shape

        # idxì™€ targetsëŠ” ëª¨ë‘ (B,T) í¬ê¸°ì˜ ì •ìˆ˜ í…ì„œì…ë‹ˆë‹¤
        tok_emb = self.token_embedding_table(idx) # (B,T,C)
        pos_emb = self.position_embedding_table(torch.arange(T, device=DEVICE)) # (T,C)
        x = tok_emb + pos_emb # (B,T,C)
        x = self.blocks(x) # (B,T,C)
        x = self.ln_f(x) # (B,T,C)
        logits = self.lm_head(x) # (B,T,vocab_size)

        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)

        return logits, loss

    def generate(self, idx: torch.Tensor, max_new_tokens: int):
        """ì›ë³¸ ëª¨ë¸ì˜ generate ë©”ì„œë“œ"""
        # idxëŠ” (B, T) í¬ê¸°ì˜ í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ ë°°ì—´ì…ë‹ˆë‹¤
        for _ in range(max_new_tokens):
            # ì»¨í…ìŠ¤íŠ¸ë¥¼ ìš°ë¦¬ì˜ ìµœëŒ€ ê¸¸ì´ë¡œ ìë¦…ë‹ˆë‹¤
            idx_cond = idx[:, -BLOCK_SIZE:]
            # ì˜ˆì¸¡ì„ ì–»ê³  ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤
            logits, loss = self(idx_cond)
            # ë§ˆì§€ë§‰ ì‹œê°„ ë‹¨ê³„ì— ì§‘ì¤‘í•˜ê³  logitsë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤
            logits = logits[:, -1, :] # (B, C)
            # softmaxë¥¼ ì ìš©í•˜ì—¬ í™•ë¥ ì„ ì–»ìŠµë‹ˆë‹¤
            probs = F.softmax(logits, dim=-1) # (B, C)
            # ë¶„í¬ì—ì„œ ë‹¤ìŒ í† í°ì„ ìƒ˜í”Œë§í•©ë‹ˆë‹¤
            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)
            # ìƒ˜í”Œë§ëœ ì¸ë±ìŠ¤ë¥¼ ì‹¤í–‰ ì¤‘ì¸ ì‹œí€€ìŠ¤ì— ì¶”ê°€í•©ë‹ˆë‹¤
            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)
        return idx

# ë°ì´í„°ì…‹ ë° ë°°ì¹˜ ìƒì„± í•¨ìˆ˜ (ì›ë³¸ê³¼ ë™ì¼)
def get_batch(split: str, train_data: torch.Tensor, val_data: torch.Tensor):
    """ì›ë³¸ ëª¨ë¸ì˜ get_batch í•¨ìˆ˜"""
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))
    x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])
    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])
    x, y = x.to(DEVICE), y.to(DEVICE)
    return x, y

@torch.no_grad()
def estimate_loss(model: nn.Module, train_data: torch.Tensor, val_data: torch.Tensor):
    """ì›ë³¸ ëª¨ë¸ì˜ estimate_loss í•¨ìˆ˜"""
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(EVAL_ITERS)
        for k in range(EVAL_ITERS):
            X, Y = get_batch(split, train_data, val_data)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

def train_model(model: nn.Module, model_name: str, train_data: torch.Tensor, val_data: torch.Tensor):
    """ëª¨ë¸ í›ˆë ¨ í•¨ìˆ˜ (ì›ë³¸ êµ¬ì¡° ê¸°ë°˜)"""
    print(f"\n=== {model_name} ëª¨ë¸ í›ˆë ¨ ì‹œì‘ ===")
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    
    # í›ˆë ¨ íˆìŠ¤í† ë¦¬
    train_losses = []
    val_losses = []
    iterations = []
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
    total_params = sum(p.numel() for p in model.parameters())
    print(f"{model_name} ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,} ({total_params/1e6:.2f}M)")
    
    for iter in tqdm(range(MAX_ITERS), desc=f"{model_name} í›ˆë ¨"):
        # í‰ê°€
        if iter % EVAL_INTERVAL == 0 or iter == MAX_ITERS - 1:
            losses = estimate_loss(model, train_data, val_data)
            train_losses.append(losses['train'].item())
            val_losses.append(losses['val'].item())
            iterations.append(iter)
            print(f"Step {iter}: Train Loss {losses['train']:.4f}, Val Loss {losses['val']:.4f}")
        
        # ë°°ì¹˜ ìƒ˜í”Œë§
        xb, yb = get_batch('train', train_data, val_data)
        
        # ìˆœì „íŒŒ
        logits, loss = model(xb, yb)
        
        # ì—­ì „íŒŒ
        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        optimizer.step()
        
        # ëª¨ë¸ ì €ì¥
        if iter % 1000 == 0 and iter > 0:
            torch.save(model.state_dict(), f'{model_name.lower()}_model_{iter}.pt')
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    torch.save(model.state_dict(), f'{model_name.lower()}_model_final.pt')
    print(f"âœ… {model_name} ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
    
    return train_losses, val_losses, iterations

def plot_training_curves(rnn_data, transformer_data, iterations):
    """í›ˆë ¨ ê³¡ì„  ì‹œê°í™”"""
    rnn_train, rnn_val = rnn_data
    transformer_train, transformer_val = transformer_data
    
    plt.figure(figsize=(15, 5))
    
    # í›ˆë ¨ ì†ì‹¤
    plt.subplot(1, 3, 1)
    plt.plot(iterations, rnn_train, label='RNN Train', color='blue', alpha=0.7)
    plt.plot(iterations, transformer_train, label='Transformer Train', color='red', alpha=0.7)
    plt.xlabel('Iterations')
    plt.ylabel('Training Loss')
    plt.title('Training Loss Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ê²€ì¦ ì†ì‹¤
    plt.subplot(1, 3, 2)
    plt.plot(iterations, rnn_val, label='RNN Validation', color='blue', alpha=0.7)
    plt.plot(iterations, transformer_val, label='Transformer Validation', color='red', alpha=0.7)
    plt.xlabel('Iterations')
    plt.ylabel('Validation Loss')
    plt.title('Validation Loss Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ê²€ì¦ ì†ì‹¤ ë¹„êµ (ë” ëª…í™•í•œ ì‹œê°í™”)
    plt.subplot(1, 3, 3)
    plt.plot(iterations, rnn_val, label='RNN', color='blue', linewidth=2)
    plt.plot(iterations, transformer_val, label='Transformer', color='red', linewidth=2)
    plt.xlabel('Iterations')
    plt.ylabel('Validation Loss')
    plt.title('Model Performance Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('training_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

def generate_text(model: nn.Module, tokenizer: BPETokenizer, prompt: str, max_tokens: int = 100):
    """í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜"""
    model.eval()
    
    # í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©
    prompt_tokens = tokenizer.encode(prompt)
    context = torch.tensor([prompt_tokens], dtype=torch.long, device=DEVICE)
    
    # í…ìŠ¤íŠ¸ ìƒì„±
    with torch.no_grad():
        generated_tokens = model.generate(context, max_new_tokens=max_tokens)
    
    # ë””ì½”ë”©
    generated_text = tokenizer.decode(generated_tokens[0].tolist())
    
    model.train()
    return generated_text

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ì–¸ì–´ ëª¨ë¸ ë¹„êµ ì‹¤í—˜ (ê³¼ì œ ìš”êµ¬ì‚¬í•­ ì¶©ì¡±)")
    print("=" * 60)
    print(f"âœ… RNN (LSTM) ëª¨ë¸")
    print(f"âœ… Transformer ëª¨ë¸") 
    print(f"âœ… ì‹¤ì œ BPE í† í¬ë‚˜ì´ì € (ì–´íœ˜ í¬ê¸°: {VOCAB_SIZE})")
    print(f"âœ… ë™ì¼í•œ ë°ì´í„°ì…‹ ì‚¬ìš©")
    print(f"âœ… ìˆ˜ë ´ê¹Œì§€ í›ˆë ¨")
    print(f"âœ… ì„±ëŠ¥ ë¹„êµ ë° ë¶„ì„")
    print("=" * 60)
    
    # BPEasy ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    if BPEasyTokenizer is not None:
        print("ğŸ¯ ì‹¤ì œ BPEasy í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤!")
    else:
        print("âš ï¸  ëŒ€ì•ˆ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (BPEasy ì‚¬ìš© ë¶ˆê°€)")
    print("=" * 60)
    
    # ë°ì´í„° ë¡œë“œ
    data_path = 'Char_Transformer_Language_Model/input.txt'
    if not os.path.exists(data_path):
        print(f"âŒ {data_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("GitHubì—ì„œ ë¦¬í¬ì§€í† ë¦¬ë¥¼ í´ë¡ í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    print(f"ğŸ“ ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}")
    with open(data_path, 'r', encoding='utf-8') as f:
        text = f.read()
    
    print(f"âœ… ë°ì´í„° í¬ê¸°: {len(text):,} ë¬¸ì")
    
    # BPE í† í¬ë‚˜ì´ì € ì„¤ì •
    tokenizer = BPETokenizer(VOCAB_SIZE)
    
    # í† í¬ë‚˜ì´ì € í›ˆë ¨ ë˜ëŠ” ë¡œë“œ
    tokenizer_path = "bpe_tokenizer.json"
    if os.path.exists(tokenizer_path):
        vocab_size = tokenizer.load_tokenizer(tokenizer_path)
    else:
        vocab_size = tokenizer.train_tokenizer(text, tokenizer_path)
    
    if vocab_size is None:
        print("í† í¬ë‚˜ì´ì € ë¡œë“œ ë˜ëŠ” í›ˆë ¨ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… ì–´íœ˜ í¬ê¸°: {vocab_size}")
    
    # ë°ì´í„° í† í°í™”
    print("ë°ì´í„° í† í°í™” ì¤‘...")
    tokens = tokenizer.encode(text)
    data = torch.tensor(tokens, dtype=torch.long)
    
    # í›ˆë ¨/ê²€ì¦ ë¶„í• 
    n = int(0.9 * len(data))
    train_data = data[:n]
    val_data = data[n:]
    
    print(f"âœ… í›ˆë ¨ ë°ì´í„°: {len(train_data):,} í† í°")
    print(f"âœ… ê²€ì¦ ë°ì´í„°: {len(val_data):,} í† í°")
    
    # ëª¨ë¸ ìƒì„±
    print("\n=== ëª¨ë¸ ìƒì„± ===")
    rnn_model = RNNAttention(vocab_size).to(DEVICE)
    transformer_model = TransformerLanguageModel(vocab_size).to(DEVICE)
    
    # ëª¨ë¸ í›ˆë ¨
    print("\n=== ëª¨ë¸ í›ˆë ¨ ===")
    rnn_train_losses, rnn_val_losses, iterations = train_model(
        rnn_model, train_data, val_data, "RNN"
    )
    
    transformer_train_losses, transformer_val_losses, iterations = train_model(
        transformer_model, train_data, val_data, "Transformer"
    )
    
    # í›ˆë ¨ ê³¡ì„  ì‹œê°í™”
    plot_training_curves(
        (rnn_train_losses, rnn_val_losses),
        (transformer_train_losses, transformer_val_losses),
        iterations
    )
    
    # í…ìŠ¤íŠ¸ ìƒì„± ë¹„êµ
    print("\n=== í…ìŠ¤íŠ¸ ìƒì„± ë¹„êµ ===")
    
    prompt = "To be or not to be"
    print(f"\ní”„ë¡¬í”„íŠ¸: '{prompt}'")
    
    print("\nRNN ëª¨ë¸ ìƒì„±:")
    rnn_text = generate_text(rnn_model, tokenizer, prompt, 100)
    print(rnn_text)
    
    print("\nTransformer ëª¨ë¸ ìƒì„±:")
    transformer_text = generate_text(transformer_model, tokenizer, prompt, 100)
    print(transformer_text)
    
    # ìµœì¢… ì„±ëŠ¥ ë¹„êµ
    print("\n=== ìµœì¢… ì„±ëŠ¥ ë¹„êµ ===")
    print(f"RNN ìµœì¢… ê²€ì¦ ì†ì‹¤: {rnn_val_losses[-1]:.4f}")
    print(f"Transformer ìµœì¢… ê²€ì¦ ì†ì‹¤: {transformer_val_losses[-1]:.4f}")
    
    if transformer_val_losses[-1] < rnn_val_losses[-1]:
        print("ğŸ† Transformer ëª¨ë¸ì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤!")
        improvement = ((rnn_val_losses[-1] - transformer_val_losses[-1]) / rnn_val_losses[-1]) * 100
        print(f"ì„±ëŠ¥ í–¥ìƒ: {improvement:.2f}%")
    else:
        print("ğŸ† RNN ëª¨ë¸ì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤!")
        improvement = ((transformer_val_losses[-1] - rnn_val_losses[-1]) / transformer_val_losses[-1]) * 100
        print(f"ì„±ëŠ¥ í–¥ìƒ: {improvement:.2f}%")
    
    # ê²°ê³¼ ë¶„ì„
    print("\n=== ê²°ê³¼ ë¶„ì„ ===")
    print("1. ëª¨ë¸ ì•„í‚¤í…ì²˜:")
    print("   - RNN (LSTM): ìˆœì°¨ì  ì²˜ë¦¬, ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµì— ì œí•œ")
    print("   - Transformer: ë³‘ë ¬ ì²˜ë¦¬, ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ ìš°ìˆ˜")
    
    print("\n2. í† í¬ë‚˜ì´ì €:")
    print(f"   - BPE ë°©ì‹ ì‚¬ìš© (ì–´íœ˜ í¬ê¸°: {vocab_size})")
    print("   - ë‹¨ì–´ ë‹¨ìœ„ í† í°í™”ë¡œ ì˜ë¯¸ ë‹¨ìœ„ í•™ìŠµ ê°€ëŠ¥")
    
    print("\n3. í›ˆë ¨ íŠ¹ì„±:")
    print("   - ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©")
    print("   - ìˆ˜ë ´ê¹Œì§€ ì¶©ë¶„í•œ í›ˆë ¨ ìˆ˜í–‰")
    print("   - ì •ê·œí™” ê¸°ë²• ì ìš© (Dropout, LayerNorm)")
    
    print("\nâœ… ì‹¤í—˜ ì™„ë£Œ!")

if __name__ == '__main__':
    main()